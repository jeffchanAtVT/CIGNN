{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore')\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "from tensorflow.contrib import legacy_seq2seq\n",
    "from tensorflow.contrib.rnn import RNNCell\n",
    "import argparse\n",
    "import yaml\n",
    "import os\n",
    "import pandas as pd\n",
    "import logging\n",
    "import numpy as np\n",
    "import pickle\n",
    "import scipy.sparse as sp\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import numpy.testing as npt\n",
    "import copy\n",
    "import datetime\n",
    "import tensorflow.contrib.slim as slim\n",
    "import inspect\n",
    "import sys\n",
    "import tqdm\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "from scipy.sparse import linalg\n",
    "from pprint import pformat\n",
    "from importlib import reload\n",
    "import cmocean\n",
    "import matplotlib.transforms as mtrans\n",
    "from lib.AMSGrad import AMSGrad\n",
    "import gc\n",
    "sns.set(rc={'figure.figsize':(40, 6)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     21,
     27,
     34,
     39,
     51,
     68,
     85,
     99,
     108,
     110
    ],
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_simple_summary(writer, names, values, global_step):\n",
    "    for name, value in zip(names, values):\n",
    "        summary = tf.Summary()\n",
    "        summary_value = summary.value.add()\n",
    "        summary_value.simple_value = value\n",
    "        summary_value.tag = name\n",
    "        writer.add_summary(summary, global_step)\n",
    "def get_total_trainable_parameter_size():\n",
    "    total_parameters = 0\n",
    "    for variable in tf.trainable_variables():\n",
    "        total_parameters += np.product([x.value for x in variable.get_shape()])\n",
    "    return total_parameters\n",
    "def load_pickle(pickle_file):\n",
    "    try:\n",
    "        with open(pickle_file, 'rb') as f:\n",
    "            pickle_data = pickle.load(f)\n",
    "    except UnicodeDecodeError as e:\n",
    "        with open(pickle_file, 'rb') as f:\n",
    "            pickle_data = pickle.load(f, encoding='latin1')\n",
    "    except Exception as e:\n",
    "        print('Unable to load data ', pickle_file, ':', e)\n",
    "        raise\n",
    "    return pickle_data\n",
    "# logger\n",
    "def config_logging(log_dir, log_filename='info.log', level=logging.INFO):\n",
    "    # Add file handler and stdout handler\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    # Create the log directory if necessary.\n",
    "    try:\n",
    "        os.makedirs(log_dir)\n",
    "    except OSError:\n",
    "        pass\n",
    "    file_handler = logging.FileHandler(os.path.join(log_dir, log_filename))\n",
    "    file_handler.setFormatter(formatter)\n",
    "    file_handler.setLevel(level=level)\n",
    "    # Add console handler.\n",
    "    console_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    console_handler = logging.StreamHandler(sys.stdout)\n",
    "    console_handler.setFormatter(console_formatter)\n",
    "    console_handler.setLevel(level=level)\n",
    "    logging.basicConfig(handlers=[file_handler, console_handler], level=level)\n",
    "def get_logger(log_dir, name, log_filename='info.log', level=logging.INFO):\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(level)\n",
    "    # Add file handler and stdout handler\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    file_handler = logging.FileHandler(os.path.join(log_dir, log_filename))\n",
    "    file_handler.setFormatter(formatter)\n",
    "    # Add console handler.\n",
    "    console_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    console_handler = logging.StreamHandler(sys.stdout)\n",
    "    console_handler.setFormatter(console_formatter)\n",
    "    logger.addHandler(file_handler)\n",
    "    logger.addHandler(console_handler)\n",
    "    # Add google cloud log handler\n",
    "    logger.info('Log directory: %s', log_dir)\n",
    "    return logger\n",
    "# matrix transformation\n",
    "def calculate_normalized_laplacian(adj):\n",
    "    \"\"\"\n",
    "    # L = D^-1/2 (D-A) D^-1/2 = I - D^-1/2 A D^-1/2\n",
    "    # D = diag(A 1)\n",
    "    :param adj:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    d = np.array(adj.sum(1))\n",
    "    d_inv_sqrt = np.power(d, -0.5).flatten()\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "    normalized_laplacian = sp.eye(adj.shape[0]) - adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n",
    "    return normalized_laplacian\n",
    "def calculate_random_walk_matrix(adj_mx):\n",
    "    adj_mx[adj_mx < 0] = 0\n",
    "    adj_mx = sp.coo_matrix(adj_mx)\n",
    "    d = np.array(adj_mx.sum(1))\n",
    "    d_inv = np.power(d, -1).flatten()\n",
    "    d_inv[np.isinf(d_inv)] = 0.\n",
    "    d_mat_inv = sp.diags(d_inv)\n",
    "    random_walk_mx = d_mat_inv.dot(adj_mx).tocoo()\n",
    "    return random_walk_mx\n",
    "def calculate_reverse_random_walk_matrix(adj_mx):\n",
    "    return calculate_random_walk_matrix(np.transpose(adj_mx))\n",
    "def calculate_scaled_laplacian(adj_mx, lambda_max=2, undirected=True):\n",
    "    adj_mx[adj_mx < 0] = 0\n",
    "    if undirected:\n",
    "        adj_mx = np.maximum.reduce([adj_mx, adj_mx.T])\n",
    "    L = calculate_normalized_laplacian(adj_mx)\n",
    "    if lambda_max is None:\n",
    "        lambda_max, _ = linalg.eigsh(L, 1, which='LM')\n",
    "        lambda_max = lambda_max[0]\n",
    "    L = sp.csr_matrix(L)\n",
    "    M, _ = L.shape\n",
    "    I = sp.identity(M, format='csr', dtype=L.dtype)\n",
    "    L = (2 / lambda_max * L) - I\n",
    "    return L.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     1,
     22
    ]
   },
   "outputs": [],
   "source": [
    "# Classes of scaler\n",
    "class MinMaxScaler:\n",
    "    def __init__(self, M=None, ran=(0,1)):\n",
    "        if M is not None:\n",
    "            _M = M\n",
    "            self.fit(_M)\n",
    "    def fit(self, M):\n",
    "        _M = M\n",
    "        self._min = np.min(_M, axis=0)\n",
    "        self._max = np.max(_M, axis=0)\n",
    "        self._num_features = _M.shape[-1]\n",
    "        assert len(self._min) == self._num_features\n",
    "        assert len(self._max) == self._num_features\n",
    "        assert np.all(self._max - self._min > 1e-6)\n",
    "    def transform(self ,M):\n",
    "        _M = M\n",
    "        _M_scale = (_M - self._min) / (self._max - self._min)\n",
    "        return _M_scale\n",
    "    def inverse_transform(self, M):\n",
    "        _M = M\n",
    "        _M_scaleback = (self._min + _M * (self._max - self._min) ) \n",
    "        return _M_scaleback\n",
    "class StandardScaler:\n",
    "    def __init__(self, M=None):\n",
    "        if M is not None:\n",
    "            _M = M\n",
    "            self.fit(_M)\n",
    "    def fit(self, M):\n",
    "        _M = M\n",
    "        self._mean = np.mean(_M, axis=0)\n",
    "        self._std = np.std(_M, axis=0)\n",
    "        self._num_features = _M.shape[-1]\n",
    "    def transform(self ,M):\n",
    "        _M = M\n",
    "        _M_scale = (_M - self._mean) / self._std\n",
    "        return _M_scale\n",
    "    def inverse_transform(self, M):\n",
    "        _M = M\n",
    "        _M_scaleback = (_M * self._std + self._mean) \n",
    "        return _M_scaleback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1,
     6,
     18,
     30,
     33,
     35,
     46,
     69,
     78,
     87,
     109
    ]
   },
   "outputs": [],
   "source": [
    "#Loss function\n",
    "def masked_mae_loss_transform(T, scaler):\n",
    "    T_transpose = tf.transpose(T, perm=[0, 1, 3, 2])\n",
    "    T_inv = scaler.inverse_transform(T_transpose)\n",
    "    T_transpose_back = tf.transpose(T_inv, perm=[0, 1, 3, 2])\n",
    "    return T_transpose_back\n",
    "def masked_mse_tf(preds, labels, null_val=np.nan):\n",
    "    if np.isnan(null_val):\n",
    "        mask = ~tf.is_nan(labels)\n",
    "    else:\n",
    "        mask = tf.not_equal(labels, null_val)\n",
    "    mask = tf.cast(mask, tf.float32)\n",
    "    mask /= tf.reduce_mean(mask)\n",
    "    mask = tf.where(tf.is_nan(mask), tf.zeros_like(mask), mask)\n",
    "    loss = tf.square(tf.subtract(preds, labels))\n",
    "    loss = loss * mask\n",
    "    loss = tf.where(tf.is_nan(loss), tf.zeros_like(loss), loss)\n",
    "    return tf.reduce_mean(loss)\n",
    "def masked_mae_tf(preds, labels, null_val=np.nan):\n",
    "    if np.isnan(null_val):\n",
    "        mask = ~tf.is_nan(labels)\n",
    "    else:\n",
    "        mask = tf.not_equal(labels, null_val)\n",
    "    mask = tf.cast(mask, tf.float32)\n",
    "    mask /= tf.reduce_mean(mask)\n",
    "    mask = tf.where(tf.is_nan(mask), tf.zeros_like(mask), mask)\n",
    "    loss = tf.abs(tf.subtract(preds, labels))\n",
    "    loss = loss * mask\n",
    "    loss = tf.where(tf.is_nan(loss), tf.zeros_like(loss), loss)\n",
    "    return tf.reduce_mean(loss)\n",
    "def masked_rmse_tf(preds, labels, null_val=np.nan):\n",
    "    return tf.sqrt(masked_mse_tf(preds=preds, labels=labels, null_val=null_val))\n",
    "def masked_rmse_np(preds, labels, null_val=np.nan):\n",
    "    return np.sqrt(masked_mse_np(preds=preds, labels=labels, null_val=null_val))\n",
    "def masked_mse_np(preds, labels, null_val=np.nan):\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        if np.isnan(null_val):\n",
    "            mask = ~np.isnan(labels)\n",
    "        else:\n",
    "            mask = np.not_equal(labels, null_val)\n",
    "        mask = mask.astype('float32')\n",
    "        mask /= np.mean(mask)\n",
    "        rmse = np.square(np.subtract(preds, labels)).astype('float32')\n",
    "        rmse = np.nan_to_num(rmse * mask)\n",
    "        return np.mean(rmse)\n",
    "def masked_mae_np(preds, labels, null_val=np.nan):\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        if np.isnan(null_val):\n",
    "            mask = ~np.isnan(labels)\n",
    "        else:\n",
    "            mask = np.not_equal(labels, null_val)\n",
    "        mask = mask.astype('float32')\n",
    "        mask /= np.mean(mask)\n",
    "        mae = np.abs(np.subtract(preds, labels)).astype('float32')\n",
    "        mae = np.nan_to_num(mae * mask)\n",
    "        return np.mean(mae)\n",
    "def masked_mape_np(preds, labels, null_val=np.nan):\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        if np.isnan(null_val):\n",
    "            mask = ~np.isnan(labels)\n",
    "        else:\n",
    "            mask = np.not_equal(labels, null_val)\n",
    "        mask = mask.astype('float32')\n",
    "        mask /= np.mean(mask)\n",
    "        mape = np.abs(np.divide(np.subtract(preds, labels).astype('float32'), labels))\n",
    "        mape = np.nan_to_num(mask * mape)\n",
    "        return np.mean(mape)\n",
    "\n",
    "def masked_mse_loss(scaler, null_val):\n",
    "    assert 0\n",
    "    def loss(preds, labels):\n",
    "        if scaler:\n",
    "            preds = scaler.inverse_transform(preds)\n",
    "            labels = scaler.inverse_transform(labels)\n",
    "        return masked_mse_tf(preds=preds, labels=labels, null_val=null_val)\n",
    "\n",
    "    return loss\n",
    "def masked_rmse_loss(scaler, null_val):\n",
    "    assert 0\n",
    "    def loss(preds, labels):\n",
    "        if scaler:\n",
    "            preds = scaler.inverse_transform(preds)\n",
    "            labels = scaler.inverse_transform(labels)\n",
    "        return masked_rmse_tf(preds=preds, labels=labels, null_val=null_val)\n",
    "\n",
    "    return loss\n",
    "def masked_mae_loss(null_val, scalers=None, multiple=False):\n",
    "    def loss(preds, labels):\n",
    "        if scalers and multiple:\n",
    "            num_scalers = len(scalers)\n",
    "            preds_per_source = tf.split(value=preds, num_or_size_splits=multiple, axis=-2)\n",
    "            labels_per_source = tf.split(value=labels, num_or_size_splits=multiple, axis=-2)\n",
    "            pred_array, label_array = [], []\n",
    "            for i, scaler in enumerate(scalers):\n",
    "                # transpose axis 2(num_nodes) and axis 3(num_features_per_nodes) before inverse transform\n",
    "                pred = masked_mae_loss_transform(preds_per_source[i], scaler)\n",
    "                label = masked_mae_loss_transform(labels_per_source[i], scaler)\n",
    "                pred_array.append(pred)\n",
    "                label_array.append(label)\n",
    "            preds = tf.concat(values=pred_array, axis=-2)\n",
    "            labels = tf.concat(values=label_array, axis=-2)\n",
    "#             preds = pred_array[0]\n",
    "#             labels = label_array[0]\n",
    "        mae = masked_mae_tf(preds=preds, labels=labels, null_val=null_val)\n",
    "        return mae\n",
    "\n",
    "    return loss\n",
    "\n",
    "def calculate_metrics(df_pred, df_test, null_val):\n",
    "    mape = masked_mape_np(preds=df_pred.as_matrix(), labels=df_test.as_matrix(), null_val=null_val)\n",
    "    mae = masked_mae_np(preds=df_pred.as_matrix(), labels=df_test.as_matrix(), null_val=null_val)\n",
    "    rmse = masked_rmse_np(preds=df_pred.as_matrix(), labels=df_test.as_matrix(), null_val=null_val)\n",
    "    return mae, mape, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     9
    ]
   },
   "outputs": [],
   "source": [
    "def create_dir_input_output_if_not_exists(create=False, **configs):\n",
    "    dp_input = configs.get('input_dir', './data')\n",
    "    dp_output = configs.get('output_dir', './results')\n",
    "    dp_input = os.path.join(dp_input, '')\n",
    "    dp_output = os.path.join(dp_output, '')\n",
    "    if create == True:\n",
    "        create_dir_if_not_exists(dp_input, recursive=True)\n",
    "        create_dir_if_not_exists(dp_output, recursive=True)\n",
    "    return dp_input, dp_output\n",
    "def create_dir_config_preprocessed_models_if_not_exists(create=False, **configs):\n",
    "    lags = configs.get('lags', 3)\n",
    "    horizon = configs.get('horizon', 3)\n",
    "    scaler_type = configs.get('Standard', 'Standard')\n",
    "    matrix_type = configs.get('matrix_type', 'Kernel')\n",
    "\n",
    "    dp_output_config = dp_output + \\\n",
    "                os.path.join('config_{}_{}_{}_{}'.format(lags, horizon, scaler_type, matrix_type), '')\n",
    "    if create == True:\n",
    "        create_dir_if_not_exists(dp_output_config)\n",
    "\n",
    "    dp_preprocessed = dp_output_config + \\\n",
    "                os.path.join('preprocessed', '')\n",
    "    dp_models = dp_output_config + \\\n",
    "                os.path.join('models', '')\n",
    "    if create == True:\n",
    "        create_dir_if_not_exists(dp_preprocessed)\n",
    "        create_dir_if_not_exists(dp_models)\n",
    "    return dp_output_config, dp_preprocessed, dp_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1,
     31,
     42,
     53,
     65,
     74,
     122
    ],
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load Data\n",
    "class DataLoader(object):\n",
    "    def __init__(self, xs, ys, batch_size, pad_with_last_sample=True, shuffle=False):\n",
    "        self.batch_size = batch_size\n",
    "        self.current_ind = 0\n",
    "        if pad_with_last_sample:\n",
    "            num_padding = (batch_size - (len(xs) % batch_size)) % batch_size\n",
    "            x_padding = np.repeat(xs[-1:], num_padding, axis=0)\n",
    "            y_padding = np.repeat(ys[-1:], num_padding, axis=0)\n",
    "            xs = np.concatenate([xs, x_padding], axis=0)\n",
    "            ys = np.concatenate([ys, y_padding], axis=0)\n",
    "        self.size = len(xs)\n",
    "        self.num_batch = int(self.size // self.batch_size)\n",
    "        if shuffle:\n",
    "            permutation = np.random.permutation(self.size)\n",
    "            xs, ys = xs[permutation], ys[permutation]\n",
    "        self.xs = xs\n",
    "        self.ys = ys\n",
    "    def get_iterator(self):\n",
    "        self.current_ind = 0\n",
    "\n",
    "        def _wrapper():\n",
    "            while self.current_ind < self.num_batch:\n",
    "                start_ind = self.batch_size * self.current_ind\n",
    "                end_ind = min(self.size, self.batch_size * (self.current_ind + 1))\n",
    "                x_i = self.xs[start_ind: end_ind, ...]\n",
    "                y_i = self.ys[start_ind: end_ind, ...]\n",
    "                yield (x_i, y_i)\n",
    "                self.current_ind += 1\n",
    "\n",
    "        return _wrapper()\n",
    "def get_dp_preprocessed(**config):\n",
    "    lags = configs.get('lags', 3)\n",
    "    horizon = configs.get('horizon', 3)\n",
    "    scaler_type = configs.get('Standard', 'Standard')\n",
    "    matrix_type = configs.get('matrix_type', 'Kernel')\n",
    "    dp_input, dp_output = create_dir_input_output_if_not_exists(**configs)\n",
    "    dp_output_config = dp_output + \\\n",
    "            os.path.join('config_{}_{}_{}_{}'.format(lags, horizon, scaler_type, matrix_type), '')\n",
    "    dp_preprocessed = dp_output_config + \\\n",
    "            os.path.join('preprocessed', '')\n",
    "    return dp_preprocessed\n",
    "def load_pickle(pickle_file):\n",
    "    try:\n",
    "        with open(pickle_file, 'rb') as f:\n",
    "            pickle_data = pickle.load(f)\n",
    "    except UnicodeDecodeError as e:\n",
    "        with open(pickle_file, 'rb') as f:\n",
    "            pickle_data = pickle.load(f, encoding='latin1')\n",
    "    except Exception as e:\n",
    "        print('Unable to load data ', pickle_file, ':', e)\n",
    "        raise\n",
    "    return pickle_data\n",
    "def load_matrix(**configs):\n",
    "    matrixs, id_map_idxs = [], []\n",
    "    dp_preprocessed = get_dp_preprocessed(**configs)\n",
    "    signals = configs.get('signals', None)\n",
    "    for signal in signals:\n",
    "        dp_signal = dp_preprocessed + \\\n",
    "                os.path.join(signal, '')\n",
    "        fp_matrix = dp_signal + 'matrix.pkl'\n",
    "        matrix, id_map_idx = load_pickle(fp_matrix)\n",
    "        matrixs.append(matrix)\n",
    "        id_map_idxs.append(id_map_idx)\n",
    "    return matrixs, id_map_idxs\n",
    "def load_scaler(fp, scaler_type):\n",
    "    scaler = load_pickle(fp)\n",
    "    if scaler_type == 'MinMax':\n",
    "        scaler._min = scaler._min.values\n",
    "        scaler._max = scaler._max.values\n",
    "    if scaler_type == 'Standard':\n",
    "        scaler._mean = scaler._mean.values\n",
    "        scaler._std = scaler._std.values\n",
    "    return scaler\n",
    "def load_signals(**configs):\n",
    "    batch_size = configs.get('batch_size', 64)\n",
    "    test_batch_size = configs.get('test_batch_size', 64)\n",
    "    signals = configs.get('signals', None)\n",
    "    lags = configs.get('lags', 3)\n",
    "    horizon = configs.get('horizon', 3)\n",
    "    scaler_type = configs.get('scaler', 'Standard')\n",
    "    matrix_type = configs.get('matrix_type', 'Kernel')\n",
    "    \n",
    "    data_heter = {}\n",
    "    for category in ['train', 'val', 'test']:\n",
    "        data_heter['x_' + category] = []\n",
    "        data_heter['y_' + category] = []\n",
    "        \n",
    "    scalers, datas, l_num_nodes = [], [], []\n",
    "    \n",
    "    for signal in signals:\n",
    "        data = {}\n",
    "        dp_preprocessed = get_dp_preprocessed(**configs)\n",
    "        dp_signal = dp_preprocessed + \\\n",
    "                os.path.join(signal, '')\n",
    "        fp_scaler = dp_signal + \\\n",
    "                '{}.pkl'.format(scaler_type)\n",
    "        scaler = load_scaler(fp_scaler, scaler_type)\n",
    "        for category in ['train', 'val', 'test']:\n",
    "            fp_cat = dp_signal + \\\n",
    "                '{}.npz'.format(category)\n",
    "            data_cat = np.load(fp_cat)\n",
    "            data['x_' + category] = data_cat['x']\n",
    "            data['y_' + category] = data_cat['y']\n",
    "            data_heter['x_' + category].append(data['x_' + category])\n",
    "            data_heter['y_' + category].append(data['y_' + category])\n",
    "        \n",
    "        scalers.append(scaler)\n",
    "        datas.append(data)\n",
    "        l_num_nodes.append(data_cat['x'].shape[-2])\n",
    "\n",
    "\n",
    "    for category in ['train', 'val', 'test']:    \n",
    "        data_heter['x_' + category] = np.concatenate(data_heter['x_' + category], axis=-2)\n",
    "        data_heter['y_' + category] = np.concatenate(data_heter['y_' + category], axis=-2)\n",
    "\n",
    "        \n",
    "    data_heter['train_loader'] = DataLoader(data_heter['x_train'], data_heter['y_train'], batch_size, shuffle=True)\n",
    "    data_heter['val_loader'] = DataLoader(data_heter['x_val'], data_heter['y_val'], test_batch_size, shuffle=False)\n",
    "    data_heter['test_loader'] = DataLoader(data_heter['x_test'], data_heter['y_test'], test_batch_size, shuffle=False)\n",
    "    datas.append(data_heter)\n",
    "    return datas, scalers, l_num_nodes\n",
    "def load_config(fp_configs):\n",
    "    assert os.path.isfile(fp_configs), '{} is not a regular file'.format(fp_configs)\n",
    "    with open(fp_configs) as f:\n",
    "        configs = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    return configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     1,
     3,
     61,
     67,
     71,
     146
    ],
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MGCell_Heter(RNNCell):\n",
    "    def call(self, inputs, **kwargs):\n",
    "        pass\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        pass\n",
    "    def __init__(self, num_units, matrixs, l_num_nodes, gc_step=2, num_proj=None,\n",
    "                 activation=tf.nn.tanh, reuse=None, filter_type=\"random_walk\", use_gc_for_ru=True):\n",
    "        super(MGCell_Heter, self).__init__(_reuse=reuse)\n",
    "        self._num_units = num_units\n",
    "        self.l_num_nodes = l_num_nodes\n",
    "        self._num_all_nodes = sum(l_num_nodes)\n",
    "        self._num_signals = len(l_num_nodes)\n",
    "        self._num_proj = num_proj\n",
    "        self._gc_step = gc_step\n",
    "        self._activation = activation\n",
    "        self._output_sizes = [x * self._num_units for x in self.l_num_nodes]\n",
    "        self._supports = []\n",
    "        self._use_gc_for_ru = use_gc_for_ru\n",
    "        for i, adj in enumerate(matrixs):\n",
    "            adj = adj.astype('float32')\n",
    "            supports = []\n",
    "            new_supports = []\n",
    "            if filter_type == \"laplacian\":\n",
    "                supports.append(calculate_scaled_laplacian(adj, lambda_max=None))\n",
    "            elif filter_type == \"random_walk\":\n",
    "                supports.append(calculate_random_walk_matrix(adj).T)\n",
    "            elif filter_type == \"dual_random_walk\":\n",
    "                supports.append(calculate_random_walk_matrix(adj).T)\n",
    "                supports.append(calculate_random_walk_matrix(adj.T).T)\n",
    "            else:\n",
    "                supports.append(calculate_scaled_laplacian(adj))\n",
    "            for support in supports:\n",
    "                new_supports.append(self._build_sparse_matrix(support))\n",
    "            self._supports.append(new_supports)\n",
    "        self.interS_weights = []\n",
    "        self.interS_bias = []\n",
    "        with tf.variable_scope(\"MG_interS_Weight\", reuse=tf.AUTO_REUSE):\n",
    "            for i in range(self._num_signals):\n",
    "                dtype = 'float32'\n",
    "                rmself_state_shape = self._num_all_nodes - self.l_num_nodes[i]\n",
    "                S_state_shape = self.l_num_nodes[i]\n",
    "                interS_weights = tf.get_variable('interS_weights_{}'.format(i),\n",
    "                    shape=[rmself_state_shape, S_state_shape],\n",
    "                    dtype=dtype,\n",
    "                    initializer=tf.contrib.layers.xavier_initializer()\n",
    "                )\n",
    "                interS_bias = tf.get_variable('self_bias_{}'.format(i),\n",
    "                    shape=S_state_shape,\n",
    "                    dtype=dtype,\n",
    "                    initializer=tf.contrib.layers.xavier_initializer()\n",
    "                )\n",
    "                self.interS_weights.append(interS_weights)\n",
    "                self.interS_bias.append(interS_bias)\n",
    "\n",
    "        with tf.variable_scope(\"projection\", reuse=tf.AUTO_REUSE):\n",
    "            self.proj_w = tf.get_variable('w',\n",
    "                    shape=[self._num_units, 1],\n",
    "                    dtype=dtype,\n",
    "                    initializer=tf.contrib.layers.xavier_initializer()\n",
    "            )\n",
    "    @staticmethod\n",
    "    def _build_sparse_matrix(L):\n",
    "        L = L.tocoo()\n",
    "        indices = np.column_stack((L.row, L.col))\n",
    "        L = tf.SparseTensor(indices, L.data, L.shape)\n",
    "        return tf.sparse_reorder(L)\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        state_size = self._num_all_nodes * self._num_units\n",
    "        return state_size\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        output_size = self._num_all_nodes * self._num_units\n",
    "        if self._num_proj is not None:\n",
    "            output_size = self._num_all_nodes * self._num_proj\n",
    "        return output_size\n",
    "\n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "        if inputs.get_shape() == state.get_shape():\n",
    "            split_inputs = tf.split(inputs, [x * self._num_units for x in self.l_num_nodes], axis=-1)\n",
    "        else:\n",
    "            split_inputs = tf.split(inputs, self.l_num_nodes, axis=-1)\n",
    "        split_state = tf.split(state, [x * self._num_units for x in self.l_num_nodes], axis=-1)\n",
    "        with tf.variable_scope(scope or \"MG_He_cell\"):\n",
    "            with tf.variable_scope(\"gates\"):  # Reset gate and update gate.\n",
    "                output_size = [2 * self._num_units] * self._num_signals\n",
    "                fn = self._gconv\n",
    "                output_split = []\n",
    "                for i in range(self._num_signals):\n",
    "                    inputs_i = split_inputs[i]\n",
    "                    state_i = split_state[i]\n",
    "                    output_size_i = output_size[i]\n",
    "                    \n",
    "                    value = tf.nn.sigmoid(fn(inputs_i, state_i, output_size_i, bias_start=1.0, si=i))\n",
    "                    value = tf.reshape(value, (-1, self.l_num_nodes[i], output_size_i))\n",
    "                    r, u = tf.split(value=value, num_or_size_splits=2, axis=-1)\n",
    "                    r = tf.reshape(r, (-1, self.l_num_nodes[i] * self._num_units))\n",
    "                    u = tf.reshape(u, (-1, self.l_num_nodes[i] * self._num_units))\n",
    "                    with tf.variable_scope(\"candidate\"):\n",
    "                        c = self._gconv(inputs_i, r * state_i, self._num_units, si=i)\n",
    "                        if self._activation is not None:\n",
    "                            c = self._activation(c)\n",
    "                    \n",
    "                    output_i = new_state_i = u * state_i + (1 - u) * c\n",
    "                    output_split.append(output_i)\n",
    "                    \n",
    "            outputs = []\n",
    "            for i in range(self._num_signals):\n",
    "                curS = output_split[i]\n",
    "                rmself_output_split = output_split[:i] + output_split[i+1:]\n",
    "                rmself_output = tf.concat(rmself_output_split, axis=-1)\n",
    "                \n",
    "                batch_size = rmself_output.shape[0]\n",
    "                rmself_reshape = tf.reshape(rmself_output, shape=[batch_size, -1, self._num_units])\n",
    "                rmself_proj = tf.matmul(rmself_reshape, self.proj_w)\n",
    "                rmself_proj_squeeze = tf.squeeze(rmself_proj)\n",
    "                toCurS = tf.matmul(rmself_proj_squeeze, self.interS_weights[i])\n",
    "                toCurS = tf.nn.bias_add(toCurS, self.interS_bias[i])\n",
    "                #toCurS = tf.nn.softmax(toCurS)\n",
    "                toCurS = tf.nn.sigmoid(toCurS)\n",
    "                toCurS_expand = tf.expand_dims(toCurS, axis=-1)\n",
    "                toCurS_expand_invproj = tf.matmul(toCurS_expand, tf.transpose(self.proj_w))\n",
    "                toCurS_invshape = tf.reshape(toCurS_expand_invproj, shape=[batch_size, -1])\n",
    "\n",
    "                T1 = curS\n",
    "                # T1 = tf.math.multiply(self.S_weights[i], T1)\n",
    "                # T1 = tf.nn.bias_add(T1, self.S_bias[i])\n",
    "                T2 = toCurS_invshape\n",
    "                T = tf.add(T1, T2)\n",
    "                T = T1\n",
    "                outputs.append(T)\n",
    "                \n",
    "            output = new_state = tf.concat(outputs, axis=-1)\n",
    "            \n",
    "            if self._num_proj is not None:\n",
    "                w = self.proj_w\n",
    "                batch_size = inputs.get_shape()[0].value\n",
    "                output = tf.reshape(new_state, shape=(-1, self._num_units))\n",
    "                output = tf.reshape(tf.matmul(output, w), shape=(batch_size, -1))\n",
    "                #output = tf.nn.relu(output)\n",
    "        return output, new_state\n",
    "\n",
    "    @staticmethod\n",
    "    def _concat(x, x_):\n",
    "        x_ = tf.expand_dims(x_, 0)\n",
    "        return tf.concat([x, x_], axis=0)\n",
    "    def _gconv(self, inputs, state, output_size, bias_start=0.0, si=-1):\n",
    "        assert si != -1\n",
    "        batch_size = inputs.get_shape()[0].value\n",
    "        inputs = tf.reshape(inputs, (batch_size, self.l_num_nodes[si], -1))\n",
    "        state = tf.reshape(state, (batch_size, self.l_num_nodes[si], -1))\n",
    "        inputs_and_state = tf.concat([inputs, state], axis=-1)\n",
    "        input_size = inputs_and_state.get_shape()[-1].value\n",
    "        dtype = inputs.dtype\n",
    "\n",
    "        x = inputs_and_state\n",
    "        x0 = tf.transpose(x, perm=[1, 2, 0])  # (num_nodes, total_arg_size, batch_size)\n",
    "        x0 = tf.reshape(x0, shape=[self.l_num_nodes[si], input_size * batch_size])\n",
    "        x = tf.expand_dims(x0, axis=0)\n",
    "        \n",
    "        scope = tf.get_variable_scope()\n",
    "        with tf.variable_scope(scope):\n",
    "            if self._gc_step == 0:\n",
    "                pass\n",
    "            else:\n",
    "                for support in self._supports[si]:\n",
    "                    x1 = tf.sparse_tensor_dense_matmul(support, x0)\n",
    "                    x = self._concat(x, x1)\n",
    "\n",
    "                    for k in range(2, self._gc_step + 1):\n",
    "                        x2 = 2 * tf.sparse_tensor_dense_matmul(support, x1) - x0\n",
    "                        x = self._concat(x, x2)\n",
    "                        x1, x0 = x2, x1\n",
    "\n",
    "            num_matrices = len(self._supports[si]) * self._gc_step + 1  # Adds for x itself.\n",
    "            x = tf.reshape(x, shape=[num_matrices, self.l_num_nodes[si], input_size, batch_size])\n",
    "            x = tf.transpose(x, perm=[3, 1, 2, 0])  # (batch_size, num_nodes, input_size, order)\n",
    "            x = tf.reshape(x, shape=[batch_size * self.l_num_nodes[si], input_size * num_matrices])\n",
    "            \n",
    "            weights = tf.get_variable(\n",
    "                'weights_{}'.format(si), [input_size * num_matrices, output_size], dtype=dtype,\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            x = tf.matmul(x, weights)  # (batch_size * self._num_nodes, output_size)\n",
    "            biases = tf.get_variable(\"bias_{}\".format(si), [output_size], dtype=dtype,\n",
    "                                     initializer=tf.constant_initializer(bias_start, dtype=dtype))\n",
    "            x = tf.nn.bias_add(x, biases)\n",
    "        return tf.reshape(x, [batch_size, self.l_num_nodes[si] * output_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MGModel(object):\n",
    "    def __init__(self, matrixs, is_training, batch_size, **configs):\n",
    "        self._loss = None\n",
    "        self._mae = None\n",
    "        self._train_op = None\n",
    "        cl_decay_steps = int(configs.get('cl_decay_steps', 1000))\n",
    "        filter_type = configs.get('filter_type', 'random_walk')\n",
    "        gc_step = int(configs.get('gc_step', 2))\n",
    "        horizon = int(configs.get('horizon', 3))\n",
    "        lags = int(configs.get('lags', 3))\n",
    "        num_rnn_layers = int(configs.get('num_rnn_layers', 2))\n",
    "        num_units = int(configs.get('num_rnn_units', 32))\n",
    "        signals = configs.get('signals', None)\n",
    "        use_curriculum_learning = bool(configs.get('use_curriculum_learning', False))\n",
    "        \n",
    "        input_dim = int(configs.get('input_dim', 1))\n",
    "        output_dim = int(configs.get('output_dim', 1))\n",
    "        l_num_nodes = configs.get('l_num_nodes')\n",
    "        num_signals = len(signals)\n",
    "        \n",
    "        num_all_nodes = sum(l_num_nodes)\n",
    "        \n",
    "        # Input (batch_size, timesteps, num_all_nodes, input_dim)\n",
    "        self._inputs = tf.placeholder(tf.float32, shape=(batch_size, lags, num_all_nodes, input_dim), name='inputs')\n",
    "        # Labels: (batch_size, timesteps, num_sensor, input_dim), same format with input except the temporal dimension.\n",
    "        self._labels = tf.placeholder(tf.float32, shape=(batch_size, horizon, num_all_nodes, output_dim), name='labels')\n",
    "        \n",
    "        cell = MGCell_Heter(num_units=num_units, matrixs=matrixs, l_num_nodes=l_num_nodes,\n",
    "                            gc_step=2, filter_type=filter_type)\n",
    "        proj_cell = MGCell_Heter(num_units=num_units, matrixs=matrixs, l_num_nodes=l_num_nodes,\n",
    "                            gc_step=2, filter_type=filter_type, num_proj=1)\n",
    "        \n",
    "        GO_SYMBOL = tf.zeros(shape=(batch_size, num_all_nodes * output_dim))\n",
    "\n",
    "        encoding_cells = [cell] * num_rnn_layers\n",
    "        decoding_cells = [cell] * (num_rnn_layers - 1) + [proj_cell]\n",
    "        encoding_cells = tf.contrib.rnn.MultiRNNCell(encoding_cells, state_is_tuple=True)\n",
    "        decoding_cells = tf.contrib.rnn.MultiRNNCell(decoding_cells, state_is_tuple=True)\n",
    "\n",
    "        global_step = tf.train.get_or_create_global_step()\n",
    "        # Outputs: (batch_size, timesteps, num_nodes, output_dim)\n",
    "        with tf.variable_scope('MG_SEQ'):\n",
    "            inputs = tf.unstack(tf.reshape(self._inputs, (batch_size, lags, num_all_nodes * input_dim)), axis=1)\n",
    "            labels = tf.unstack(\n",
    "                tf.reshape(self._labels[..., :output_dim], (batch_size, horizon, num_all_nodes * output_dim)), axis=1)\n",
    "            labels.insert(0, GO_SYMBOL)\n",
    "\n",
    "            def _loop_function(prev, i):\n",
    "                if is_training:\n",
    "                    # Return either the model's prediction or the previous ground truth in training.\n",
    "                    if use_curriculum_learning:\n",
    "                        c = tf.random_uniform((), minval=0, maxval=1.)\n",
    "                        threshold = self._compute_sampling_threshold(global_step, cl_decay_steps)\n",
    "                        result = tf.cond(tf.less(c, threshold), lambda: labels[i], lambda: prev)\n",
    "                    else:\n",
    "                        result = labels[i]\n",
    "                else:\n",
    "                    # Return the prediction of the model in testing.\n",
    "                    result = prev\n",
    "                return result\n",
    "\n",
    "            _, enc_state = tf.contrib.rnn.static_rnn(encoding_cells, inputs, dtype=tf.float32)\n",
    "            outputs, final_state = legacy_seq2seq.rnn_decoder(labels, enc_state, decoding_cells,\n",
    "                                                              loop_function=_loop_function)\n",
    "\n",
    "        # Project the output to output_dim.\n",
    "        outputs = tf.stack(outputs[:horizon], axis=1)\n",
    "        self._outputs = tf.reshape(outputs, (batch_size, horizon, num_all_nodes, output_dim), name='outputs')\n",
    "        self._merged = tf.summary.merge_all()\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_sampling_threshold(global_step, k):\n",
    "        return tf.cast(k / (k + tf.exp(global_step / k)), tf.float32)\n",
    "    @property\n",
    "    def inputs(self):\n",
    "        return self._inputs\n",
    "    @property\n",
    "    def labels(self):\n",
    "        return self._labels\n",
    "    @property\n",
    "    def loss(self):\n",
    "        return self._loss\n",
    "    @property\n",
    "    def mae(self):\n",
    "        return self._mae\n",
    "    @property\n",
    "    def merged(self):\n",
    "        return self._merged\n",
    "    @property\n",
    "    def outputs(self):\n",
    "        return self._outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     1,
     68,
     118,
     261
    ],
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MGSupervisor(object):\n",
    "    def __init__(self, writer, **configs):\n",
    "        self._configs = configs\n",
    "        self._train_configs = configs.get('train')\n",
    "        self._signals = configs.get('signals')\n",
    "        # logging.\n",
    "        self._log_dir = self._get_log_dir(configs)\n",
    "        log_level = self._configs.get('log_level', 'INFO')\n",
    "        self._logger = get_logger(self._log_dir, __name__, 'info.log', level=log_level)\n",
    "        self._writer = writer\n",
    "        \n",
    "        # Data preparation\n",
    "        self._matrixs, self._id_to_idxs = load_matrix(**configs)\n",
    "        self._datas, self._scalers, self._l_num_nodes = load_signals(**configs)\n",
    "        self._configs.update({'l_num_nodes': self._l_num_nodes})\n",
    "\n",
    "        merged_summary = tf.summary.merge_all()\n",
    "        \n",
    "        train_batch_size = configs.get('train_batch_size')\n",
    "        test_batch_size  = configs.get('test_batch_size')\n",
    "        with tf.name_scope('Train'):\n",
    "            with tf.variable_scope('MG', reuse=False):\n",
    "                self._train_model = MGModel(is_training=True, matrixs=self._matrixs,\n",
    "                                              batch_size=train_batch_size, **self._configs)\n",
    "            with tf.variable_scope('MG', reuse=True):\n",
    "                self._test_model = MGModel(is_training=False, matrixs=self._matrixs,\n",
    "                                              batch_size=test_batch_size, **self._configs)\n",
    "\n",
    "        # Learning rate.\n",
    "        self._lr = tf.get_variable('learning_rate', shape=(), initializer=tf.constant_initializer(0.01),\n",
    "                                   trainable=False)\n",
    "        self._new_lr = tf.placeholder(tf.float32, shape=(), name='new_learning_rate')\n",
    "        self._lr_update = tf.assign(self._lr, self._new_lr, name='lr_update')\n",
    "\n",
    "        # Configure optimizer\n",
    "        optimizer_name = self._train_configs.get('optimizer', 'adam').lower()\n",
    "        epsilon = float(self._train_configs.get('epsilon', 1e-3))\n",
    "        optimizer = tf.train.AdamOptimizer(self._lr, epsilon=epsilon)\n",
    "        if optimizer_name == 'sgd':\n",
    "            optimizer = tf.train.GradientDescentOptimizer(self._lr, )\n",
    "        elif optimizer_name == 'amsgrad':\n",
    "            optimizer = AMSGrad(self._lr, epsilon=epsilon)\n",
    "\n",
    "        # Calculate loss\n",
    "        output_dim = configs.get('output_dim')\n",
    "        preds = self._train_model.outputs\n",
    "        labels = self._train_model.labels[..., :output_dim]\n",
    "\n",
    "        null_val = 0.\n",
    "        self._loss_fn = masked_mae_loss(null_val=null_val, scalers=self._scalers, multiple=self._l_num_nodes)\n",
    "        self._train_loss = self._loss_fn(preds=preds, labels=labels)\n",
    "\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads = tf.gradients(self._train_loss, tvars)\n",
    "        max_grad_norm = configs['train'].get('max_grad_norm', 1.)\n",
    "        grads, _ = tf.clip_by_global_norm(grads, max_grad_norm)\n",
    "        global_step = tf.train.get_or_create_global_step()\n",
    "        self._train_op = optimizer.apply_gradients(zip(grads, tvars), global_step=global_step, name='train_op')\n",
    "\n",
    "        max_to_keep = self._train_configs.get('max_to_keep', 100)\n",
    "        self._epoch = 0\n",
    "        self._saver = tf.train.Saver(tf.global_variables(), max_to_keep=max_to_keep)\n",
    "\n",
    "        # logging.\n",
    "        total_trainable_parameter = get_total_trainable_parameter_size()\n",
    "        self._logger.info('Total number of trainable parameters: {:d}'.format(total_trainable_parameter))\n",
    "        for var in tf.global_variables():\n",
    "            self._logger.debug('{}, {}'.format(var.name, var.get_shape()))\n",
    "    def run_epoch_generator(self, sess, model, data_generator, return_output=False, training=False, writer=None, loader=None):\n",
    "        losses = []\n",
    "        maes = []\n",
    "        outputs = []\n",
    "        output_dim = self._configs.get('output_dim')\n",
    "        preds = model.outputs\n",
    "        labels = model.labels[..., :output_dim]\n",
    "        loss = self._loss_fn(preds=preds, labels=labels)\n",
    "        fetches = {\n",
    "            'loss': loss,\n",
    "            'mae': loss,\n",
    "            'global_step': tf.train.get_or_create_global_step()\n",
    "        }\n",
    "        if training:\n",
    "            fetches.update({\n",
    "                'train_op': self._train_op\n",
    "            })\n",
    "            merged = model.merged\n",
    "            if merged is not None:\n",
    "                fetches.update({'merged': merged})\n",
    "\n",
    "        if return_output:\n",
    "            fetches.update({\n",
    "                'outputs': model.outputs\n",
    "            })\n",
    "\n",
    "        for _, (x, y) in enumerate(data_generator):\n",
    "            feed_dict = {\n",
    "                model.inputs: x,\n",
    "                model.labels: y,\n",
    "            }\n",
    "\n",
    "            vals = sess.run(fetches, feed_dict=feed_dict)\n",
    "\n",
    "            losses.append(vals['loss'])\n",
    "            maes.append(vals['mae'])\n",
    "            \n",
    "            if writer is not None and 'merged' in vals:\n",
    "                writer.add_summary(vals['merged'], global_step=vals['global_step'])\n",
    "            if return_output:\n",
    "                outputs.append(vals['outputs'])\n",
    "\n",
    "        results = {\n",
    "            'loss': np.mean(losses),\n",
    "            'mae': np.mean(maes)\n",
    "        }\n",
    "        if return_output:\n",
    "            results['outputs'] = outputs\n",
    "        return results\n",
    "\n",
    "    def train(self, sess, **configs):\n",
    "        configs.update(self._train_configs)\n",
    "        return self._train(sess, **configs)\n",
    "    def _train(self, sess, base_lr, epoch, steps, patience=10, epochs=40,\n",
    "               min_learning_rate=2e-6, lr_decay_ratio=0.1, save_model=1,\n",
    "               test_every_n_epochs=10, **train_configs):\n",
    "        history = []\n",
    "        min_val_loss = float('inf')\n",
    "        wait = 0\n",
    "        max_to_keep = train_configs.get('max_to_keep', 100)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=max_to_keep)\n",
    "        model_filename = train_configs.get('model_filename')\n",
    "        if model_filename is not None:\n",
    "            saver.restore(sess, model_filename)\n",
    "            self._epoch = epoch + 1\n",
    "        else:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "        self._logger.info('Start training ...')\n",
    "\n",
    "        # Force epoch to be zero\n",
    "        # self._epoch = 0\n",
    "        while self._epoch <= epochs:\n",
    "            # Learning rate schedule.\n",
    "            new_lr = max(min_learning_rate, base_lr * (lr_decay_ratio ** np.sum(self._epoch >= np.array(steps))))\n",
    "            self.set_lr(sess=sess, lr=new_lr)\n",
    "\n",
    "            start_time = time.time()\n",
    "            train_results = self.run_epoch_generator(sess, self._train_model,\n",
    "                                                     self._datas[-1]['train_loader'].get_iterator(),\n",
    "                                                     training=True,\n",
    "                                                     writer=self._writer)\n",
    "\n",
    "            train_loss, train_mae = train_results['loss'], train_results['mae']\n",
    "            if train_loss > 1e5:\n",
    "                self._logger.warning('Gradient explosion detected. Ending...')\n",
    "                break\n",
    "\n",
    "            global_step = sess.run(tf.train.get_or_create_global_step())\n",
    "            # Compute validation error.\n",
    "            val_results = self.run_epoch_generator(sess, self._test_model,\n",
    "                                                   self._datas[-1]['val_loader'].get_iterator(),\n",
    "                                                   training=False)\n",
    "            val_loss, val_mae = val_results['loss'], val_results['mae']\n",
    "            add_simple_summary(self._writer,\n",
    "                                     ['loss/train_loss', 'metric/train_mae', 'loss/val_loss', 'metric/val_mae'],\n",
    "                                     [train_loss, train_mae, val_loss, val_mae], global_step=global_step)\n",
    "            end_time = time.time()\n",
    "            message = 'Epoch [{}/{}] ({}) train_mae: {:.4f}, val_mae: {:.4f} lr:{:.6f} {:.1f}s'.format(\n",
    "                self._epoch, epochs, global_step, train_mae, val_mae, new_lr, (end_time - start_time))\n",
    "            self._logger.info(message)\n",
    "            if self._epoch % test_every_n_epochs == test_every_n_epochs - 1:\n",
    "                outputs = self.evaluate(sess)\n",
    "                self.outputs = outputs\n",
    "                mobile_pred_1 = outputs['predictions'][0][0].squeeze()\n",
    "                mobile_grou_1 = outputs['groundtruth'][0][0].squeeze()\n",
    "                temp_pred_1 = outputs['predictions'][1][0].squeeze()\n",
    "                temp_grou_1 = outputs['groundtruth'][1][0].squeeze()\n",
    "                plot_my_pred_ground(mobile_pred_1, mobile_grou_1)\n",
    "                # plot_my_pred_ground(temp_pred_1, temp_grou_1, num_plots=3)\n",
    "            if val_loss <= min_val_loss:\n",
    "                wait = 0\n",
    "                if save_model > 0:\n",
    "                    model_filename = self.save(sess, val_loss)\n",
    "                self._logger.info(\n",
    "                    'Val loss decrease from %.4f to %.4f, saving to %s' % (min_val_loss, val_loss, model_filename))\n",
    "                min_val_loss = val_loss\n",
    "            else:\n",
    "                wait += 1\n",
    "                if wait > patience:\n",
    "                    self._logger.warning('Early stopping at epoch: %d' % self._epoch)\n",
    "                    break\n",
    "\n",
    "            history.append(val_mae)\n",
    "            self._epoch += 1\n",
    "\n",
    "            sys.stdout.flush()\n",
    "        return np.min(history)\n",
    "\n",
    "    def evaluate(self, sess, **configs):\n",
    "        global_step = sess.run(tf.train.get_or_create_global_step())\n",
    "        test_results = self.run_epoch_generator(sess, self._test_model,\n",
    "                                                self._datas[-1]['test_loader'].get_iterator(),\n",
    "                                                return_output=True,\n",
    "                                                training=False)\n",
    "\n",
    "        # y_preds:  a list of (batch_size, horizon, num_nodes, output_dim)\n",
    "        test_loss, y_preds = test_results['loss'], test_results['outputs']\n",
    "        add_simple_summary(self._writer, ['loss/test_loss'], [test_loss], global_step=global_step)\n",
    "\n",
    "        y_preds = np.concatenate(y_preds, axis=0)\n",
    "        predictions_conc = []\n",
    "        y_truths_conc = []\n",
    "        \n",
    "        start_idx = 0\n",
    "        for i, signal in enumerate(self._signals):\n",
    "            predictions = []\n",
    "            y_truths = []\n",
    "            \n",
    "            self._logger.info(\"Graph: {}\".format(signal))\n",
    "            num_nodes = self._l_num_nodes[i]\n",
    "\n",
    "            end_idx = start_idx + num_nodes\n",
    "            for horizon_i in range(self._datas[-1]['y_test'].shape[1]):\n",
    "\n",
    "                y_truth = self._datas[i]['y_test'][:, horizon_i, :, :]\n",
    "                y_pred = y_preds[:y_truth.shape[0], horizon_i, start_idx:end_idx, :]\n",
    "\n",
    "                if len(self._scalers) > 0:\n",
    "                    Shape = y_truth.shape\n",
    "                    y_truth = y_truth.squeeze()\n",
    "                    y_truth = self._scalers[i].inverse_transform(y_truth)\n",
    "                    y_truth = np.reshape(y_truth, Shape)\n",
    "                    Shape = y_pred.shape\n",
    "                    y_pred = y_pred.squeeze()\n",
    "                    y_pred = self._scalers[i].inverse_transform(y_pred)\n",
    "                    y_pred = np.reshape(y_pred, Shape)\n",
    "\n",
    "                y_truths.append(y_truth)\n",
    "                predictions.append(y_pred)\n",
    "                \n",
    "                mae = masked_mae_np(y_pred, y_truth, null_val=0)\n",
    "                rmse = masked_rmse_np(y_pred, y_truth, null_val=0)\n",
    "                self._logger.info(\n",
    "                    \"Horizon {:02d}, MAE: {:.2f}, RMSE: {:.2f}, MAPE: {:.4f}\".format(\n",
    "                        horizon_i + 1, mae, rmse, mape\n",
    "                    )\n",
    "                )\n",
    "                add_simple_summary(self._writer,\n",
    "                                         ['%s_%d' % (item, horizon_i + 1) for item in\n",
    "                                          ['metric/rmse', 'metric/mape', 'metric/mae']],\n",
    "                                         [rmse, mape, mae],\n",
    "                                         global_step=global_step)\n",
    "            start_idx = end_idx\n",
    "            predictions_conc.append(predictions)\n",
    "            y_truths_conc.append(y_truths)\n",
    "        outputs = {\n",
    "            'predictions': predictions_conc,\n",
    "            'groundtruth': y_truths_conc\n",
    "        }\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "    def load(self, sess, model_filename):\n",
    "        print(\"resoring from saved model\")\n",
    "        self._saver.restore(sess, model_filename)\n",
    "    def save(self, sess, val_loss):\n",
    "        configs = dict(self._configs)\n",
    "        global_step = np.asscalar(sess.run(tf.train.get_or_create_global_step()))\n",
    "        prefix = os.path.join(self._log_dir, 'models-{:.4f}'.format(val_loss))\n",
    "        configs['train']['epoch'] = self._epoch\n",
    "        configs['train']['global_step'] = global_step\n",
    "        configs['train']['log_dir'] = self._log_dir\n",
    "        configs['train']['model_filename'] = self._saver.save(sess, prefix, global_step=global_step,\n",
    "                                                             write_meta_graph=False)\n",
    "        fn_configs = 'config_{}.yaml'.format(self._epoch)\n",
    "        with open(os.path.join(self._log_dir, fn_configs), 'w') as f:\n",
    "            yaml.dump(configs, f, default_flow_style=False)\n",
    "        return configs['train']['model_filename']\n",
    "    def get_lr(self, sess):\n",
    "        return np.asscalar(sess.run(self._lr))\n",
    "    def set_lr(self, sess, lr):\n",
    "        sess.run(self._lr_update, feed_dict={\n",
    "            self._new_lr: lr\n",
    "        })\n",
    "    @staticmethod\n",
    "    def _get_log_dir(configs):\n",
    "        log_dir = configs['train'].get('log_dir')\n",
    "        if log_dir is None:\n",
    "            batch_size = configs.get('train_batch_size')\n",
    "            gc_step = configs.get('gc_step')\n",
    "            learning_rate = configs['train'].get('base_lr')\n",
    "            num_rnn_layers = configs.get('num_rnn_layers')\n",
    "            num_rnn_units = configs.get('num_rnn_units')\n",
    "            structure = '-'.join(\n",
    "                ['%d' % num_rnn_units for _ in range(num_rnn_layers)])\n",
    "            \n",
    "            filter_type = configs.get('filter_type')\n",
    "            filter_type_abbr = 'L'\n",
    "            if filter_type == 'random_walk':\n",
    "                filter_type_abbr = 'R'\n",
    "            elif filter_type == 'dual_random_walk':\n",
    "                filter_type_abbr = 'DR'\n",
    "            run_id = 'MG_%s_%d_%s_lr_%g_bs_%d_%s/' % (\n",
    "                filter_type_abbr, gc_step,\n",
    "                structure, learning_rate, batch_size,\n",
    "                time.strftime('%m%d%H%M%S'))\n",
    "            \n",
    "            lags = configs.get('lags', 3)\n",
    "            horizon = configs.get('horizon', 3)\n",
    "            scaler_type = configs.get('Standard', 'Standard')\n",
    "            matrix_type = configs.get('matrix_type', 'Kernel')\n",
    "            dp_output = configs.get('output_dir')\n",
    "            dp_output = os.path.join(dp_output, '')\n",
    "            dp_output_config = dp_output + \\\n",
    "                    os.path.join('config_{}_{}_{}_{}'.format(lags, horizon, scaler_type, matrix_type), 'models')\n",
    "            log_dir = os.path.join(dp_output_config, run_id)\n",
    "        if not os.path.exists(log_dir):\n",
    "            os.makedirs(log_dir)\n",
    "        return log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     6
    ],
    "tags": []
   },
   "outputs": [],
   "source": [
    "arguments = {\n",
    "    \"fp_configs\": './config.yaml',\n",
    "    \"only_cpu\" : True,\n",
    "}\n",
    "tf_config = tf.ConfigProto()\n",
    "if arguments[\"only_cpu\"] == False:\n",
    "    tf_config = tf.ConfigProto(device_count={'GPU': 0})\n",
    "    tf_config.gpu_options.allow_growth = True\n",
    "configs = load_config(arguments['fp_configs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.shutdown()\n",
    "reload(logging)\n",
    "tf.reset_default_graph()\n",
    "sess = tf.Session(config=tf_config)\n",
    "writer = tf.summary.FileWriter(\"/tmp/MG_graph/2\")\n",
    "supervisor = MGSupervisor(writer=writer, **configs)\n",
    "supervisor.train(sess=sess)\n",
    "outputs = supervisor.evaluate(sess=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p36-Cloud",
   "language": "python",
   "name": "p36-cloud"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
